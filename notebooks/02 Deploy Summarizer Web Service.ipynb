{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to an Azure Machine Learning Workspace\n",
    "\n",
    "The Azure Machine Learning Python SDK is required for leveraging the experimentation, model management and model deployment capabilities of Azure Machine Learning services. Run the following cell to create a new Azure Machine Learning **Workspace** and save the configuration to disk. The configuration file named `config.json` is saved in a folder named `.azureml`. \n",
    "\n",
    "**Important Note**: You might be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print('azureml.core.VERSION: ', azureml.core.VERSION)\n",
    "\n",
    "# import the Workspace class and check the azureml SDK version\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print(ws)\n",
    "print('Workspace configuration succeeded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model to Azure Container Instance (ACI)\n",
    "\n",
    "In this section, you will deploy a web service that uses Gensim as shown in `01 Summarize` to summarize text. The web service will be hosted in Azure Container Service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the scoring web service\n",
    "\n",
    "When deploying models for scoring with Azure Machine Learning services, you need to define the code for a simple web service that will load your model and use it for scoring. By convention this service has two methods init which loads the model and run which scores data using the loaded model.\n",
    "\n",
    "This scoring service code will later be deployed inside of a specially prepared Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile summarizer_service.py\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "from gensim.summarization import summarize, keywords\n",
    "\n",
    "def clean_and_parse_document(document):\n",
    "    if isinstance(document, str):\n",
    "        document = document\n",
    "    elif isinstance(document, unicode):\n",
    "        return unicodedata.normalize('NFKD', document).encode('ascii', 'ignore')\n",
    "    else:\n",
    "        raise ValueError(\"Document is not string or unicode.\")\n",
    "    document = document.strip()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "def summarize_text(text, summary_ratio=None, word_count=30):\n",
    "    sentences = clean_and_parse_document(text)\n",
    "    cleaned_text = ' '.join(sentences)\n",
    "    summary = summarize(cleaned_text, split=True, ratio=summary_ratio, word_count=word_count)\n",
    "    return summary \n",
    "\n",
    "def init():  \n",
    "    nltk.download('punkt')\n",
    "    return\n",
    "\n",
    "def run(input_str):\n",
    "    try:\n",
    "        return summarize_text(input_str)\n",
    "    except Exception as e:\n",
    "        return (str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "Azure ML environments are an encapsulation of the environment where your machine learning training happens. They define Python packages, environment variables, Docker settings and other attributes in declarative fashion. Environments are versioned: you can update them and retrieve old versions to revisit and review your work.\n",
    "\n",
    "Environments allow you to:\n",
    "* Encapsulate dependencies of your training process, such as Python packages and their versions.\n",
    "* Reproduce the Python environment on your local computer in a remote run on VM or ML Compute cluster\n",
    "* Reproduce your experimentation environment in production setting.\n",
    "* Revisit and audit the environment in which an existing model was trained.\n",
    "\n",
    "Environment, compute target and training script together form run configuration: the full specification of training run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use curated environments\n",
    "\n",
    "Curated environments are provided by Azure Machine Learning and are available in your workspace by default. They contain collections of Python packages and settings to help you get started different machine learning frameworks. \n",
    "\n",
    "  * The __AzureML-Minimal__ environment contains a minimal set of packages to enable run tracking and asset uploading. You can use it as a starting point for your own environment.\n",
    "  * The __AzureML-Tutorial__ environment contains common data science packages, such as Scikit-Learn, Pandas and Matplotlib, and larger set of azureml-sdk packages.\n",
    " \n",
    "Curated environments are backed by cached Docker images, reducing the run preparation cost.\n",
    "\n",
    "See https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/using-environments/using-environments.ipynb for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own environment\n",
    "\n",
    "Instead of curated environments, may create an environment by instantiating ```Environment``` object and then setting its attributes: set of Python packages, environment variables and others.  We will take this approach in this training.\n",
    "\n",
    "#### Add Python packages\n",
    "\n",
    "The recommended way is to specify Conda packages, as they typically come with complete set of pre-built binaries. Still, you may also add pip packages, and specify the version of package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.environment import CondaDependencies\n",
    "\n",
    "myenv = Environment(name=\"myenv\")\n",
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_pip_package(\"gensim==3.8.3\")\n",
    "conda_dep.add_pip_package(\"nltk==3.4.5\")\n",
    "myenv.python.conda_dependencies=conda_dep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register environment\n",
    "\n",
    "You can manage environments by registering them. This allows you to track their versions, and reuse them in future runs. For example, once you've constructed an environment that meets your requirements, you can register it and use it in other experiments so as to standardize your workflow.\n",
    "\n",
    "If you register the environment with same name, the version number is increased by one. Note that Azure ML keeps track of differences between the version, so if you re-register an identical version, the version number is not increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "If you want more control over how your model is run, if it uses another framework, or if it has special runtime requirements, you can instead specify your own environment and scoring method. Custom environments can be used for any model you want to deploy.\n",
    "\n",
    "In previous code, you specified the model's runtime environment by creating an [Environment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment%28class%29?view=azure-ml-py) object and providing the [CondaDependencies](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py) needed by your model.\n",
    "\n",
    "In the following cells you will use the Azure Machine Learning SDK to package the model and scoring script in a container, and deploy that container to an Azure Container Instance.\n",
    "\n",
    "Run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='summarizer_service.py', environment=myenv)\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores = 1, \n",
    "    memory_gb = 1, \n",
    "    tags = {'name':'Summarization'}, \n",
    "    description = 'Summarizes text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to begin your deployment to the Azure Container Instance.  In this case, the model is inside the Python script, so the _model_ parameter has a blank list.\n",
    "\n",
    "Run the following cell:  you may be waiting 5-15 minutes for completion, while the _Running_ tag adds progress dots.\n",
    "\n",
    "You will see output similar to the following when your web service is ready: \n",
    "\n",
    "`\n",
    "Succeeded\n",
    "ACI service creation operation finished, operation \"Succeeded\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "from azureml.core import Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "service_name = \"summarizer\"\n",
    "\n",
    "# Remove any existing service under the same name.\n",
    "try:\n",
    "    Webservice(ws, service_name).delete()\n",
    "except WebserviceException:\n",
    "    pass\n",
    "\n",
    "webservice = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[],\n",
    "                       inference_config=inference_config,\n",
    "                       overwrite=True,\n",
    "                       deployment_config=aci_config)\n",
    "webservice.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the deployed service\n",
    "\n",
    "Now you are ready to test scoring using the deployed web service. The following cell invokes the web service.\n",
    "\n",
    "Run the following cells to test scoring using a single input row against the deployed web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_document = \"\"\"\n",
    "I was driving down El Camino and stopped at a red light.\n",
    "It was about 3pm in the afternoon.  \n",
    "The sun was bright and shining just behind the stoplight.\n",
    "This made it hard to see the lights.\n",
    "There was a car on my left in the left turn lane.\n",
    "A few moments later another car, a black sedan pulled up behind me. \n",
    "When the left turn light changed green, the black sedan hit me thinking \n",
    "that the light had changed for us, but I had not moved because the light \n",
    "was still red.\n",
    "After hitting my car, the black sedan backed up and then sped past me.\n",
    "I did manage to catch its license plate. \n",
    "The license plate of the black sedan was ABC123. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = webservice.run(input_data = example_document)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture the scoring URI\n",
    "\n",
    "In order to call the service from a REST client, you need to acquire the scoring URI. Run the following cell to retrieve the scoring URI and take note of this value, you will need it in the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webservice.scoring_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default settings used in deploying this service result in a service that does not require authentication, so the scoring URI is the only value you need to call this service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
