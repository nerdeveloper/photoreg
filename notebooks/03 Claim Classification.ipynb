{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claims classification with Keras: The Python Deep Learning Library\n",
    "\n",
    "In this notebook, you will train a classification model for claim text that will predict `1` if the claim is an auto insurance claim or `0` if it is a home insurance claim. The model will be built using a type of Deep Neural Network (DNN) called the Long Short-Term Memory (LSTM) recurrent neural network using TensorFlow via the Keras library.\n",
    "\n",
    "This notebook will walk you through the text analytic process that consists of:\n",
    "\n",
    "- Example word analogy with Glove word embeddings\n",
    "- Vectorizing training data using GloVe word embeddings\n",
    "- Remotely train a LSTM based classifier model on Azure ML Compute\n",
    "- Monitor training metrics in real-time\n",
    "- Using the model to predict classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare modules\n",
    "\n",
    "This notebook will use the Keras library to build and train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, optimizers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print('Keras version: ', keras.__version__)\n",
    "print('Tensorflow version: ', tf.__version__)\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Run, Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "print(\"Azure ML SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's download the pretrained GloVe word embeddings and load them in this notebook.**\n",
    "\n",
    "This will create a `dictionary` of size **400,000** words, and the corresponding `GloVe word vectors` for words in the dictionary. Each word vector is of size: 50, thus the dimensionality of the word embeddings used here is **50**.\n",
    "\n",
    "*The next cell might take couple of minutes to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                  'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "\n",
    "word_vectors_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordVectors.npy')\n",
    "\n",
    "word_vectors_dir = './word_vectors'\n",
    "\n",
    "os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "urllib.request.urlretrieve(word_vectors_url, os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "\n",
    "dictionary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "dictionary = dictionary.tolist()\n",
    "dictionary = [word.decode('UTF-8') for word in dictionary]\n",
    "print('Loaded the dictionary! Dictionary size: ', len(dictionary))\n",
    "\n",
    "word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the word contractions map. The map is going to used to expand contractions in our corpus (for example \"can't\" becomes \"cannot\").**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n",
    "contractions_df = pd.read_excel(contractions_url)\n",
    "contractions = dict(zip(contractions_df.original, contractions_df.expanded))\n",
    "print('Review first 10 entries from the contractions map')\n",
    "print(contractions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word analogy example with GloVe word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe represents each word in the dictionary as a vector. We can use word vectors for predicting word analogies. \n",
    "\n",
    "See example below that solves the following analogy: **father->mother :: king->?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is a measure used to evaluate how similar two words are. This helper function takes vectors of two words and returns their cosine similarity that range from -1 to 1. For synonyms the cosine similarity will be close to 1 and for antonyms the cosine similarity will be close to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    dot = u.dot(v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    cosine_similarity = dot/norm_u/norm_v\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s review the vector for the words **father**, **mother**, and **king**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "father = word_vectors[dictionary.index('father')]\n",
    "mother = word_vectors[dictionary.index('mother')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "print(father)\n",
    "print('')\n",
    "print(mother)\n",
    "print('')\n",
    "print(king)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve for the analogy, we need to solve for x in the following equation:\n",
    "\n",
    "**mother – father = x - king**\n",
    "\n",
    "Thus, **x = mother - father + king**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mother - father + king"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we will find the word whose word vector is closest to the vector x computed above**\n",
    "\n",
    "To limit the computation cost, we will identify the best word from a list of possible answers instead of searching the entire dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = ['women', 'prince', 'princess', 'england', 'goddess', 'diva', 'empress', \n",
    "           'female', 'lady', 'monarch', 'title', 'queen', 'sovereign', 'ruler', \n",
    "           'male', 'crown', 'majesty', 'royal', 'cleopatra', 'elizabeth', 'victoria', \n",
    "           'throne', 'internet', 'sky', 'machine', 'learning', 'fairy']\n",
    "\n",
    "df = pd.DataFrame(columns = ['word', 'cosine_similarity'])\n",
    "\n",
    "# Find the similarity of each word in answers with x\n",
    "for w in answers:\n",
    "    sim = cosine_similarity(word_vectors[dictionary.index(w)], x)   \n",
    "    df = df.append({'word': w, 'cosine_similarity': sim}, ignore_index=True)\n",
    "    \n",
    "df.sort_values(['cosine_similarity'], ascending=False, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the results above, you can observe the vector for the word `queen` is most similar to the vector `x`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training data\n",
    "\n",
    "Contoso Ltd has provided a small document containing examples of the text they receive as claim text. They have provided this in a text file with one line per sample claim.\n",
    "\n",
    "Run the following cell to download and examine the contents of the file. Take a moment to read the claims (you may find some of them rather comical!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = './data'\n",
    "base_data_url = 'https://databricksdemostore.blob.core.windows.net/data/05.03/'\n",
    "filesToDownload = ['claims_text.txt', 'claims_labels.txt']\n",
    "\n",
    "os.makedirs(data_location, exist_ok=True)\n",
    "\n",
    "for file in filesToDownload:\n",
    "    data_url = os.path.join(base_data_url, file)\n",
    "    local_file_path = os.path.join(data_location, file)\n",
    "    urllib.request.urlretrieve(data_url, local_file_path)\n",
    "    print('Downloaded file: ', file)\n",
    "    \n",
    "claims_corpus = [claim for claim in open(os.path.join(data_location, 'claims_text.txt'))]\n",
    "claims_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the claims sample, Contoso Ltd has also provided a document that labels each of the sample claims provided as either 0 (\"home insurance claim\") or 1 (\"auto insurance claim\"). This to is presented as a text file with one row per sample, presented in the same order as the claim text.\n",
    "\n",
    "Run the following cell to examine the contents of the supplied claims_labels.txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(re.sub(\"\\n\", \"\", label)) for label in open(os.path.join(data_location, 'claims_labels.txt'))]\n",
    "print(len(labels))\n",
    "print(labels[0:5]) # first 5 labels\n",
    "print(labels[-5:]) # last 5 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above output, the values are integers 0 or 1. In order to use these as labels with which to train our model, we need to convert these integer values to categorical values (think of them like enum's from other programming languages).\n",
    "\n",
    "We can use the to_categorical method from `keras.utils` to convert these value into binary categorical values. Run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, 2)\n",
    "print(labels.shape)\n",
    "print()\n",
    "print(labels[0:2]) # first 2 categorical labels\n",
    "print()\n",
    "print(labels[-2:]) # last 2 categorical labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our claims text and labels loaded, we are ready to begin our first step in the text analytics process, which is to normalize the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the claims corpus\n",
    "\n",
    "- Lowercase all words\n",
    "- Expand contractions (for example \"can't\" becomes \"cannot\")\n",
    "- Remove special characters (like punctuation)\n",
    "- Convert the list of words in the claims text to a list of corresponding indices of those words in the dictionary. Note that the order of the words as they appear in the written claims is maintained.\n",
    "\n",
    "Run the next cell to process the claims corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(token):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_token = pattern.sub('', token)\n",
    "    return filtered_token\n",
    "\n",
    "def convert_to_indices(corpus, dictionary, c_map, unk_word_index = 399999):\n",
    "    sequences = []\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = corpus[i].split()\n",
    "        sequence = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word in c_map:\n",
    "                resolved_words = c_map[word].split()\n",
    "                for resolved_word in resolved_words:\n",
    "                    try:\n",
    "                        word_index = dictionary.index(resolved_word)\n",
    "                        sequence.append(word_index)\n",
    "                    except ValueError:\n",
    "                        sequence.append(unk_word_index) #Vector for unkown words\n",
    "            else:\n",
    "                try:\n",
    "                    clean_word = remove_special_characters(word)\n",
    "                    if len(clean_word) > 0:\n",
    "                        word_index = dictionary.index(clean_word)\n",
    "                        sequence.append(word_index)\n",
    "                except ValueError:\n",
    "                    sequence.append(unk_word_index) #Vector for unkown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "claims_corpus_indices = convert_to_indices(claims_corpus, dictionary, contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review the indices of one sample claim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print('Ordered list of indices for the above claim')\n",
    "print(claims_corpus_indices[5])\n",
    "print('')\n",
    "print('For example, the index of second word in the claims text \\\"pedestrian\\\" is: ', dictionary.index('pedestrian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create fixed length vectors**\n",
    "\n",
    "The number of words used in a claim, vary with the claim. We need to create the input vectors of fixed size. We will use the utility function `pad_sequences` from `keras.preprocessing.sequence` to help us create fixed size vector (size = 125) of word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 125\n",
    "\n",
    "X = pad_sequences(claims_corpus_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n",
    "\n",
    "print('Review the new fixed size vector for a sample claim')\n",
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print(X[5])\n",
    "print('')\n",
    "print('Lenght of the vector: ', len(X[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload training data to blob store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the `Workspace` from the saved config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(azureml.core.VERSION)\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload features and labels to the default blob store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_location = './inputs'\n",
    "features_file = os.path.join(input_location, 'features.npy')\n",
    "labels_file = os.path.join(input_location, 'labels.npy')\n",
    "os.makedirs(input_location, exist_ok=True)\n",
    "np.save(features_file, X)\n",
    "np.save(labels_file, labels)\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(input_location, \n",
    "                 target_path = 'inputs', \n",
    "                 overwrite = True, \n",
    "                 show_progress = True)\n",
    "\n",
    "# Create a file dataset object that represents the features and the labels files\n",
    "dataset = Dataset.File.from_files((datastore, 'inputs/*.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remotely Train the DNN using the Azure ML Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Compute Cluster\n",
    "\n",
    "Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's create a new Aml Compute in the current workspace, if it doesn't already exist. We will run the model training jobs on this compute target. This will take couple of minutes to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create AML CPU based Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"amlcompute-ad\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6',\n",
    "                                                           min_nodes=1, max_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training script\n",
    "\n",
    "The training script builds and trains the DNN. Review the code below to understand the structure of the neural network. As you will see, in this case we will build a LSTM recurrent neural network. The network will have a word embedding layer that will convert the word indices to GloVe word vectors. The GloVe word vectors are then passed to the LSTM layer, followed by a binary classifier output layer.\n",
    "\n",
    "Run the next two cells to create and save the training script in the `scripts` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_file_folder = './scripts'\n",
    "script_file_name = 'train.py'\n",
    "script_file_full_path = os.path.join(script_file_folder, script_file_name)\n",
    "os.makedirs(script_file_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_file_full_path\n",
    "import os\n",
    "import argparse\n",
    "import urllib.request\n",
    "import math\n",
    "import timeit\n",
    "import numpy as np\n",
    "np.random.seed(437)\n",
    "\n",
    "print(\"numpy version: {}\".format(np.__version__))\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model as KModel #resolve name conflict with Azure Model\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "print(\"keras version: {} tensorflow version: {} sklearn version: {}\".format(keras.__version__, \n",
    "                                                                            tensorflow.__version__, \n",
    "                                                                            sklearn.__version__))\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--input-folder\", type=str, dest='input_folder', help=\"Reference to inputs\")\n",
    "parser.add_argument('--batch-size', type=int, dest='batch_size', default=16, help='mini batch size for training')\n",
    "parser.add_argument('--lr', type=float, dest='lr', default=0.0005, help='Learning rate')\n",
    "parser.add_argument('--epochs', type=int, dest='epochs', default=25, help='Epochs')\n",
    "\n",
    "args = parser.parse_args()\n",
    "input_location = args.input_folder\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "epochs = args.epochs\n",
    "\n",
    "print('Input folder', input_location)\n",
    "print('Learning rate', lr)\n",
    "print('Batch size', batch_size)\n",
    "print('Epochs', epochs)\n",
    "\n",
    "words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                  'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "word_vectors_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordVectors.npy')\n",
    "word_vectors_dir = './word_vectors'\n",
    "os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "urllib.request.urlretrieve(word_vectors_url, os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "dictionary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "dictionary = dictionary.tolist()\n",
    "dictionary = [word.decode('UTF-8') for word in dictionary]\n",
    "print('Loaded the dictionary! Dictionary size: ', len(dictionary))\n",
    "word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)\n",
    "\n",
    "maxSeqLength = 125\n",
    "embedding_layer = Embedding(word_vectors.shape[0],\n",
    "                            word_vectors.shape[1],\n",
    "                            weights=[word_vectors],\n",
    "                            input_length=maxSeqLength,\n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "print('Reading input files...')\n",
    "features_file = os.path.join(input_location, 'features.npy')\n",
    "labels_file = os.path.join(input_location, 'labels.npy')\n",
    "X = np.load(features_file)\n",
    "labels = np.load(labels_file)\n",
    "print('Done reading input files.')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "run = Run.get_context()\n",
    "class LogRunMetrics(Callback):\n",
    "    # callback at the end of every epoch\n",
    "    def on_epoch_end(self, epoch, log):\n",
    "        # log a value repeated which creates a list\n",
    "        run.log('Loss', log['val_loss'])\n",
    "        run.log('Accuracy', log['val_accuracy'])\n",
    "_callbacks = [LogRunMetrics()]\n",
    "\n",
    "print(\"Model training starting...\")\n",
    "start_time = timeit.default_timer()\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=_callbacks,\n",
    "                    verbose=2)\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(\"Model training completed.\")\n",
    "print('Elapsed time (min): ', round(elapsed_time/60.0,0))\n",
    "\n",
    "# save the model\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "model.save(os.path.join('./outputs', 'final_model.hdf5'))\n",
    "# save training history\n",
    "with open(os.path.join('./outputs', 'history.txt'), 'w') as f:\n",
    "    f.write(str(history.history))\n",
    "\n",
    "print(\"Model saved in ./outputs folder\")\n",
    "print(\"Saving model files completed.\")\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ScriptRunConfig with custom TensforFlow 2.0 Enviroment\n",
    "\n",
    "In this case we pass the following parameters to the training script:\n",
    "\n",
    "- **input-folder**: Path to the training dataset mounted on the remote compute\n",
    "- **lr**: Learning rate for the optimizer\n",
    "- **batch-size**: Training batch size\n",
    "- **epochs**: Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.environment import CondaDependencies\n",
    "tensorflow_env = Environment.get(workspace=ws, name='AzureML-TensorFlow-2.2-CPU').clone('Custom-TensforFlow-Env')\n",
    "cd = tensorflow_env.python.conda_dependencies\n",
    "cd.add_pip_package('numpy==1.18.5')\n",
    "cd.add_pip_package('scikit-learn==0.23.1')\n",
    "tensorflow_env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_file_folder, \n",
    "                      script=script_file_name, \n",
    "                      arguments=['--input-folder', dataset.as_named_input('inputs').as_mount(),\n",
    "                                 '--lr', 0.001,\n",
    "                                 '--batch-size', 16,\n",
    "                                 '--epochs', 100], \n",
    "                      compute_target=compute_target, \n",
    "                      environment=tensorflow_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the training run\n",
    "\n",
    "The code pattern to submit a training run to Azure Machine Learning compute targets is always:\n",
    "\n",
    "- Create an experiment to run.\n",
    "- Submit the experiment.\n",
    "- Wait for the run to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'claims-classification-exp'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "run = experiment.submit(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the Run Metrics\n",
    "\n",
    "Using the azureml Jupyter widget, you can monitor the training run. You can monitor the validation accuracy and validation loss in real-time as the training progresses.\n",
    "\n",
    "The training will approximately take around 2-4 minutes to complete. Once the training is completed you can then download the trained models locally by running the **Download the trained models** cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Jupyter widget `Output Logs` drop down, select **azureml-logs/70_driver_log.txt** to review the model training output. Take a look at the final output for the value \"val_accuracy\". This stands for validation set accuracy. If you think of random chance as having a 50% accuracy, is you model better than random?\n",
    "\n",
    "It's OK if it's not much better then random at this point- this is only your first model! The typical data science process would continue with many more iterations taking different actions to improve the model accuracy, including:\n",
    "- Acquiring more labeled documents for training\n",
    "- Regularization to prevent overfitting\n",
    "- Adjusting the model hyperparameters, such as the number of layers, number of nodes per layer, and learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the trained model and the model history files\n",
    "\n",
    "> **Important**: Ensure the training run in the previous cell has completed before running the following cell. You should see a message that the \"Run is completed\" in the output logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_outputs = './outputs'\n",
    "os.makedirs(local_outputs, exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs'):\n",
    "        output_file_path = os.path.join(local_outputs, f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test classifying claims\n",
    "\n",
    "Now that you have constructed a model, try it out against a set of claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(os.path.join(local_outputs, 'final_model.hdf5'))\n",
    "print('Model loaded!')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following cell to prepare our test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claim = ['I crashed my car into a pole.', \n",
    "              'The flood ruined my house.', \n",
    "              'I lost control of my car and fell in the river.']\n",
    "\n",
    "test_claim_indices = convert_to_indices(test_claim, dictionary, contractions)\n",
    "test_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now use the model to predict the classification**\n",
    "\n",
    "Recall the classification model for claim text predicts a value of `1` if the claim is an auto insurance claim or `0` if it is a home insurance claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred_label = pred.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\n",
    "pred_df.label = pred_df.label.astype(int)\n",
    "print('Predictions')\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
