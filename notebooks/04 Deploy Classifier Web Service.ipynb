{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a Keras model to ONNX\n",
    "\n",
    "In the steps that follow, you will convert Keras model you just trained to the ONNX format. This will enable you to use this model for classification in a very broad range of environments, outside of Azure Databricks including:\n",
    "\n",
    "- Web services\n",
    "- iOS and Android mobile apps\n",
    "- Windows apps\n",
    "- IoT devices\n",
    "\n",
    "Furthermore, ONNX runtimes and libraries are also designed to maximize performance on some of the best hardware in the industry. In this lab, we will compare the Inference performance of the ONNX vs Keras models.\n",
    "\n",
    "First we will load the trained Keras model from file, and then convert the model to ONNX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Keras Model\n",
    "\n",
    "Load the saved Keras model. We will convert the Keras model to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(125)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "output_folder = './outputs'\n",
    "model_filename = 'final_model.hdf5'\n",
    "\n",
    "keras_model = load_model(os.path.join(output_folder, model_filename))\n",
    "print(keras_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to ONNX\n",
    "\n",
    "Convert the loaded Keras model to ONNX format, and save the ONNX model to the deployment folder.\n",
    "\n",
    "*Ignore the WARNING - Can't import tf2onnx module, so the conversion on a model with any custom/lambda layer will fail!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxmltools\n",
    "\n",
    "deployment_folder = 'deploy'\n",
    "onnx_export_folder = 'onnx'\n",
    "\n",
    "# Convert the Keras model to ONNX\n",
    "onnx_model_name = 'claim_classifier.onnx'\n",
    "converted_model = onnxmltools.convert_keras(keras_model, onnx_model_name, target_opset=7)\n",
    "\n",
    "# Save the model locally...\n",
    "onnx_model_path = os.path.join(deployment_folder, onnx_export_folder)\n",
    "os.makedirs(onnx_model_path, exist_ok=True)\n",
    "onnxmltools.utils.save_model(converted_model, os.path.join(onnx_model_path,onnx_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Inference using the ONNX Model\n",
    "\n",
    "- Create an ONNX runtime InferenceSession\n",
    "- Review the expected input shape to make inferences\n",
    "- Prepare test data\n",
    "- Make inferences using both the ONNX and the Keras Model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Runtime InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "# Load the ONNX model and observe the expected input shape\n",
    "onnx_session = onnxruntime.InferenceSession(\n",
    "    os.path.join(os.path.join(deployment_folder, onnx_export_folder), onnx_model_name))\n",
    "input_name = onnx_session.get_inputs()[0].name\n",
    "output_name = onnx_session.get_outputs()[0].name\n",
    "print('Expected input shape: ', onnx_session.get_inputs()[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the GloVe word vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_dir = './word_vectors'\n",
    "\n",
    "dictonary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "dictonary = dictonary.tolist()\n",
    "dictonary = [word.decode('UTF-8') for word in dictonary]\n",
    "print('Loaded the dictonary! Dictonary size: ', len(dictonary))\n",
    "\n",
    "word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the word contractions map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n",
    "contractions_df = pd.read_excel(contractions_url)\n",
    "contractions = dict(zip(contractions_df.original, contractions_df.expanded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup the helper functions to process the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def remove_special_characters(token):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_token = pattern.sub('', token)\n",
    "    return filtered_token\n",
    "\n",
    "def convert_to_indices(corpus, dictonary, c_map, unk_word_index = 399999):\n",
    "    sequences = []\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = corpus[i].split()\n",
    "        sequence = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word in c_map:\n",
    "                resolved_words = c_map[word].split()\n",
    "                for resolved_word in resolved_words:\n",
    "                    try:\n",
    "                        word_index = dictonary.index(resolved_word)\n",
    "                        sequence.append(word_index)\n",
    "                    except ValueError:\n",
    "                        sequence.append(unk_word_index) #Vector for unkown words\n",
    "            else:\n",
    "                try:\n",
    "                    clean_word = remove_special_characters(word)\n",
    "                    if len(clean_word) > 0:\n",
    "                        word_index = dictonary.index(clean_word)\n",
    "                        sequence.append(word_index)\n",
    "                except ValueError:\n",
    "                    sequence.append(unk_word_index) #Vector for unkown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "maxSeqLength = 125\n",
    "\n",
    "test_claim = ['I crashed my car into a pole.']\n",
    "\n",
    "test_claim_indices = convert_to_indices(test_claim, dictonary, contractions)\n",
    "test_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n",
    "\n",
    "# convert the data type to float\n",
    "test_data_float = np.reshape(test_data.astype(np.float32), (1,maxSeqLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Inferences\n",
    "\n",
    "Make inferences using both the ONNX and the Keras Model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an ONNX session to classify the sample.\n",
    "print('ONNX prediction: ', onnx_session.run([output_name], {input_name : test_data_float}))\n",
    "\n",
    "# Use Keras to make predictions on the same sample\n",
    "print('Keras prediction: ', keras_model.predict(test_data_float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Inference Performance: ONNX vs Keras\n",
    "\n",
    "Evaluate the performance of ONNX and Keras by running the same sample 1,000 times. Run the next three cells and compare the performance in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will compare the performance of ONNX vs Keras\n",
    "import timeit\n",
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "for i in range(n):\n",
    "    keras_model.predict(test_data_float)\n",
    "keras_elapsed = timeit.default_timer() - start_time\n",
    "print('Keras performance: ', keras_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "for i in range(n):\n",
    "    onnx_session.run([output_name], {input_name : test_data_float})\n",
    "onnx_elapsed = timeit.default_timer() - start_time\n",
    "print('ONNX performance: ', onnx_elapsed)\n",
    "print('ONNX is about {} times faster than Keras'.format(round(keras_elapsed/onnx_elapsed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy ONNX model to Azure Container Instance (ACI)\n",
    "\n",
    "Note that **Azure Kubernetes Service (AKS)** is used for high-scale production deployments. For the purposes of this lab, we will deploy to **ACI** that is typically used for low-scale CPU-based workloads that require less than 48 GB of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to an Azure Machine Learning Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(azureml.core.VERSION)\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model with Azure Machine Learning\n",
    "\n",
    "In the following, you register the model with Azure Machine Learning (which saves a copy in the cloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the model and vectorizer\n",
    "from azureml.core.model import Model\n",
    "\n",
    "registered_model_name = 'claim_classifier_onnx'\n",
    "onnx_model_path = os.path.join(os.path.join(deployment_folder, onnx_export_folder), onnx_model_name)\n",
    "\n",
    "registered_model = Model.register(model_path = onnx_model_path, # this points to a local file\n",
    "                       model_name = registered_model_name, # this is the name the model is registered with         \n",
    "                       description = \"Claims classification model.\",\n",
    "                       workspace = ws)\n",
    "\n",
    "print(registered_model.name, registered_model.description, registered_model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the scoring web service\n",
    "\n",
    "When deploying models for scoring with Azure Machine Learning services, you need to define the code for a simple web service that will load your model and use it for scoring. By convention this service has two methods init which loads the model and run which scores data using the loaded model.\n",
    "\n",
    "This scoring service code will later be deployed inside of a specially prepared Docker container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the scoring web service Python file**\n",
    "\n",
    "Note that the scoring web service needs the registered model: the ONNX model to make inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scoring_service.py\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from azureml.core.model import Model\n",
    "import onnxruntime\n",
    "\n",
    "def init():\n",
    "\n",
    "    global onnx_session\n",
    "    global dictonary\n",
    "    global contractions\n",
    "    \n",
    "    try:\n",
    "        words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                          'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "        word_vectors_dir = './word_vectors'\n",
    "        os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "        urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "        dictonary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "        dictonary = dictonary.tolist()\n",
    "        dictonary = [word.decode('UTF-8') for word in dictonary]\n",
    "        print('Loaded the dictonary! Dictonary size: ', len(dictonary))\n",
    "        \n",
    "        contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n",
    "        contractions_df = pd.read_excel(contractions_url)\n",
    "        contractions = dict(zip(contractions_df.original, contractions_df.expanded))\n",
    "        print('Loaded contractions!')\n",
    "        \n",
    "        # Retrieve the path to the model file using the model name\n",
    "        onnx_model_name = 'claim_classifier_onnx'\n",
    "        onnx_model_path = Model.get_model_path(onnx_model_name)\n",
    "        print('onnx_model_path: ', onnx_model_path)\n",
    "        \n",
    "        onnx_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "        print('Onnx Inference Session Created!')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def remove_special_characters(token):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_token = pattern.sub('', token)\n",
    "    return filtered_token\n",
    "\n",
    "def convert_to_indices(corpus, dictonary, c_map, unk_word_index = 399999):\n",
    "    sequences = []\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = corpus[i].split()\n",
    "        sequence = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word in c_map:\n",
    "                resolved_words = c_map[word].split()\n",
    "                for resolved_word in resolved_words:\n",
    "                    try:\n",
    "                        word_index = dictonary.index(resolved_word)\n",
    "                        sequence.append(word_index)\n",
    "                    except ValueError:\n",
    "                        sequence.append(unk_word_index) #Vector for unkown words\n",
    "            else:\n",
    "                try:\n",
    "                    clean_word = remove_special_characters(word)\n",
    "                    if len(clean_word) > 0:\n",
    "                        word_index = dictonary.index(clean_word)\n",
    "                        sequence.append(word_index)\n",
    "                except ValueError:\n",
    "                    sequence.append(unk_word_index) #Vector for unkown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        print(\"Received input: \", raw_data)\n",
    "        \n",
    "        maxSeqLength = 125\n",
    "        \n",
    "        print('Processing input...')\n",
    "        input_data_raw = np.array(json.loads(raw_data))\n",
    "        input_data_indices = convert_to_indices(input_data_raw, dictonary, contractions)\n",
    "        input_data_padded = pad_sequences(input_data_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n",
    "        # convert the data type to float\n",
    "        input_data = np.reshape(input_data_padded.astype(np.float32), (1,maxSeqLength))\n",
    "        print('Done processing input.')\n",
    "        \n",
    "        # Run an ONNX session to classify the input.\n",
    "        result = onnx_session.run(None, {onnx_session.get_inputs()[0].name: input_data})[0].argmax(axis=1).item()\n",
    "        # return just the classification index (0 or 1)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Model and deploy to ACI\n",
    "\n",
    "Your scoring service can have dependencies install by using a Conda environment file. Items listed in this file will be conda or pip installed within the Docker container that is created and thus be available to your scoring web service logic.\n",
    "\n",
    "The recommended deployment pattern is to create a deployment configuration object with the `deploy_configuration` method and then use it with the deploy method of the [Model](https://docs.microsoft.com/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py) class as performed below. In this case, we use the `AciWebservice`'s `deploy_configuration` and specify the CPU cores and memory size.\n",
    "\n",
    "You will see output similar to the following when your web service is ready: `Succeeded - ACI service creation operation finished, operation \"Succeeded\"`\n",
    "\n",
    "Run the following cell. This may take between 5-10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Conda dependencies environment file\n",
    "print(\"Creating conda dependencies file locally...\")\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "conda_packages = ['numpy==1.18.5', 'xlrd==1.2.0', 'pandas==1.0.4', 'scikit-learn==0.23.1']\n",
    "pip_packages = ['azureml-defaults', 'azureml-sdk', 'tensorflow==2.2.0', 'onnxruntime==1.4.0']\n",
    "\n",
    "environment = Environment('my-environment')\n",
    "environment.python.conda_dependencies = CondaDependencies.create(conda_packages=conda_packages, \n",
    "                                                                 pip_packages=pip_packages)\n",
    "\n",
    "execution_script = 'scoring_service.py'\n",
    "service_name = \"claimclassservice\"\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=execution_script, environment=environment)\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=1,\n",
    "    memory_gb=1, \n",
    "    tags = {'name': 'Claim Classification'}, \n",
    "    description = \"Classifies a claim as home or auto.\")\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                      name=service_name,\n",
    "                      models=[registered_model],\n",
    "                      inference_config=inference_config,\n",
    "                      deployment_config=aci_config)\n",
    "\n",
    "# wait for the deployment to finish\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make direct calls on the service object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_claims = ['I crashed my car into a pole.', \n",
    "               'The flood ruined my house.', \n",
    "               'I lost control of my car and fell in the river.']\n",
    "\n",
    "for i in range(len(test_claims)):\n",
    "    result = service.run(json.dumps([test_claims[i]]))\n",
    "    print('Predicted label for test claim #{} is {}'.format(i+1, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make HTTP calls to test the deployed Web Service\n",
    "\n",
    "In order to call the service from a REST client, you need to acquire the scoring URI. Take a note of printed scoring URI, you will need it in the last notebook.\n",
    "\n",
    "The default settings used in deploying this service result in a service that does not require authentication, so the scoring URI is the only value you need to call this service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = service.scoring_uri\n",
    "print('ACI Service: Claim Classification scoring URI is: {}'.format(url))\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "for i in range(len(test_claims)):\n",
    "    response = requests.post(url, json.dumps([test_claims[i]]), headers=headers)\n",
    "    print('Predicted label for test claim #{} is {}'.format(i+1, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
